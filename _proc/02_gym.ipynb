{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: gym.html\n",
    "title: Gym Environment\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've been able to implement the basic functionalities of the game in Python, our next step is to implement it as a `gym.Env` so that it can be used easily to train reinforcement learning models. As a starting point, we will be following the docs: https://www.gymlibrary.dev/content/environment_creation/.\n",
    "\n",
    "They remind us to add the `metadata` attribute to specify the render-mode (`human`, `rgb_array` or `ansi`) and the framerate. Every environment should support the render-mode `None`, and you don't need to add it explicitly.\n",
    "\n",
    "As we have almost defined the environment completelly before, we don't need to add a lot of information to this class (we can inherit from the one we defined before); but we have to explicitly define the attributes `self.observation_space` and `self.action_space`.\n",
    "\n",
    "- `self.action_space`: Our agents can only choose them column in which they want to place the dice, so our action space is going to be restricted to a number between 0 and 2 (assuming the board has 3 columns, but could depend on it directly).\n",
    "\n",
    "- `self.observation_space`: What does an agent see? It makes sense to provide all the information available: Its current board, the opponent's board and the dice it has to place. We can implement this easily with a `spaces.Dict`. The different boards can be encoded as `spaces.Box` with `dtype=np.uint8` so that they are discrete environments by with an array-like shape. It should work very similarly with a `spaces.MultiDiscrete` environment for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Jorgvt/matatena/blob/main/matatena/gym.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MatatenaEnv\n",
       "\n",
       ">      MatatenaEnv (*args, **kwds)\n",
       "\n",
       "`gym`-ready implementation of `Game`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Jorgvt/matatena/blob/main/matatena/gym.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MatatenaEnv\n",
       "\n",
       ">      MatatenaEnv (*args, **kwds)\n",
       "\n",
       "`gym`-ready implementation of `Game`."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(MatatenaEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Player 1 (0.0) | Player 2 (0.0) *\n",
       "[[0. 0. 0.]    | [[0. 0. 0.]     \n",
       " [0. 0. 0.]    |  [0. 0. 0.]     \n",
       " [0. 0. 0.]]   |  [0. 0. 0.]]    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena = MatatenaEnv()\n",
    "matatena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('agent',\n",
       "              array([[6, 1, 3],\n",
       "                     [6, 3, 6],\n",
       "                     [3, 6, 5]], dtype=uint8)),\n",
       "             ('dice', 2),\n",
       "             ('opponent',\n",
       "              array([[2, 1, 5],\n",
       "                     [1, 5, 3],\n",
       "                     [6, 2, 2]], dtype=uint8))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset\n",
    "\n",
    "The `reset` method will be called to initiate a new episode. It should be called as well when  a `done` signal is issued by the environment to reset it. It must accept a `reset` parameter. \n",
    "\n",
    "It is recommended to use the random generator included when inheriting from `gym.Env`(`self.np_random`), but we need to remember to call `super().reset(seed=seed)` to make sure that the environment is seeded correctly. \n",
    "\n",
    "Finally, it must return a tuple of the initial observation and some auxiliary information (which will be `None` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Jorgvt/matatena/blob/main/matatena/gym.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MatatenaEnv.reset\n",
       "\n",
       ">      MatatenaEnv.reset (seed:int=None, options=None)\n",
       "\n",
       "Reinitializes the environment and returns the initial state.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| seed | int | None | Seed to control the RNG. |\n",
       "| options | NoneType | None | Additional options. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Jorgvt/matatena/blob/main/matatena/gym.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MatatenaEnv.reset\n",
       "\n",
       ">      MatatenaEnv.reset (seed:int=None, options=None)\n",
       "\n",
       "Reinitializes the environment and returns the initial state.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| seed | int | None | Seed to control the RNG. |\n",
       "| options | NoneType | None | Additional options. |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(MatatenaEnv.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Player 1 (0.0) * | Player 2 (0.0)\n",
       "[[0. 0. 0.]      | [[0. 0. 0.]   \n",
       " [0. 0. 0.]      |  [0. 0. 0.]   \n",
       " [0. 0. 0.]]     |  [0. 0. 0.]]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena = MatatenaEnv()\n",
    "matatena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': array([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       "  'opponent': array([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       "  'dice': 4},\n",
       " None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Player 1 (0.0) * | Player 2 (0.0)\n",
       "[[0. 0. 0.]      | [[0. 0. 0.]   \n",
       " [0. 0. 0.]      |  [0. 0. 0.]   \n",
       " [0. 0. 0.]]     |  [0. 0. 0.]]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step\n",
    "\n",
    "The `.step()` method contains the logic of the environment. Must accept an `action`, compute the state of the environment after applying the `action` and return a 4-tuple: `(observation, reward, done, info)`.\n",
    "\n",
    "> In our case, the `action` should be the column in which the agent wants to place the rolled dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Jorgvt/matatena/blob/main/matatena/gym.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MatatenaEnv.step\n",
       "\n",
       ">      MatatenaEnv.step (action)\n",
       "\n",
       "Run one timestep of the environment's dynamics.\n",
       "\n",
       "When end of episode is reached, you are responsible for calling :meth:`reset` to reset this environment's state.\n",
       "Accepts an action and returns either a tuple `(observation, reward, terminated, truncated, info)`, or a tuple\n",
       "(observation, reward, done, info). The latter is deprecated and will be removed in future versions.\n",
       "\n",
       "Args:\n",
       "    action (ActType): an action provided by the agent\n",
       "\n",
       "Returns:\n",
       "    observation (object): this will be an element of the environment's :attr:`observation_space`.\n",
       "        This may, for instance, be a numpy array containing the positions and velocities of certain objects.\n",
       "    reward (float): The amount of reward returned as a result of taking the action.\n",
       "    terminated (bool): whether a `terminal state` (as defined under the MDP of the task) is reached.\n",
       "        In this case further step() calls could return undefined results.\n",
       "    truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied.\n",
       "        Typically a timelimit, but could also be used to indicate agent physically going out of bounds.\n",
       "        Can be used to end the episode prematurely before a `terminal state` is reached.\n",
       "    info (dictionary): `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).\n",
       "        This might, for instance, contain: metrics that describe the agent's performance state, variables that are\n",
       "        hidden from observations, or individual reward terms that are combined to produce the total reward.\n",
       "        It also can contain information that distinguishes truncation and termination, however this is deprecated in favour\n",
       "        of returning two booleans, and will be removed in a future version.\n",
       "\n",
       "    (deprecated)\n",
       "    done (bool): A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.\n",
       "        A done signal may be emitted for different reasons: Maybe the task underlying the environment was solved successfully,\n",
       "        a certain timelimit was exceeded, or the physics simulation has entered an invalid state.\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| action | Action to be executed on the environment. Should be the column in which the agent wants to place the dice. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Jorgvt/matatena/blob/main/matatena/gym.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MatatenaEnv.step\n",
       "\n",
       ">      MatatenaEnv.step (action)\n",
       "\n",
       "Run one timestep of the environment's dynamics.\n",
       "\n",
       "When end of episode is reached, you are responsible for calling :meth:`reset` to reset this environment's state.\n",
       "Accepts an action and returns either a tuple `(observation, reward, terminated, truncated, info)`, or a tuple\n",
       "(observation, reward, done, info). The latter is deprecated and will be removed in future versions.\n",
       "\n",
       "Args:\n",
       "    action (ActType): an action provided by the agent\n",
       "\n",
       "Returns:\n",
       "    observation (object): this will be an element of the environment's :attr:`observation_space`.\n",
       "        This may, for instance, be a numpy array containing the positions and velocities of certain objects.\n",
       "    reward (float): The amount of reward returned as a result of taking the action.\n",
       "    terminated (bool): whether a `terminal state` (as defined under the MDP of the task) is reached.\n",
       "        In this case further step() calls could return undefined results.\n",
       "    truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied.\n",
       "        Typically a timelimit, but could also be used to indicate agent physically going out of bounds.\n",
       "        Can be used to end the episode prematurely before a `terminal state` is reached.\n",
       "    info (dictionary): `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).\n",
       "        This might, for instance, contain: metrics that describe the agent's performance state, variables that are\n",
       "        hidden from observations, or individual reward terms that are combined to produce the total reward.\n",
       "        It also can contain information that distinguishes truncation and termination, however this is deprecated in favour\n",
       "        of returning two booleans, and will be removed in a future version.\n",
       "\n",
       "    (deprecated)\n",
       "    done (bool): A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.\n",
       "        A done signal may be emitted for different reasons: Maybe the task underlying the environment was solved successfully,\n",
       "        a certain timelimit was exceeded, or the physics simulation has entered an invalid state.\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| action | Action to be executed on the environment. Should be the column in which the agent wants to place the dice. |"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(MatatenaEnv.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Render`\n",
    "\n",
    "> Lastly, only rendering the environment is left.\n",
    "\n",
    "As we have previously built a quite decent `__repr__` method, we are going to only use that one. It would be nice to get something nicer runnig with *PyGame*, tho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Jorgvt/matatena/blob/main/matatena/gym.py#L98){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MatatenaEnv.render\n",
       "\n",
       ">      MatatenaEnv.render ()\n",
       "\n",
       "Compute the render frames as specified by render_mode attribute during initialization of the environment.\n",
       "\n",
       "The set of supported modes varies per environment. (And some\n",
       "third-party environments may not support rendering at all.)\n",
       "By convention, if render_mode is:\n",
       "\n",
       "- None (default): no render is computed.\n",
       "- human: render return None.\n",
       "  The environment is continuously rendered in the current display or terminal. Usually for human consumption.\n",
       "- single_rgb_array: return a single frame representing the current state of the environment.\n",
       "  A frame is a numpy.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image.\n",
       "- rgb_array: return a list of frames representing the states of the environment since the last reset.\n",
       "  Each frame is a numpy.ndarray with shape (x, y, 3), as with single_rgb_array.\n",
       "- ansi: Return a list of strings (str) or StringIO.StringIO containing a\n",
       "  terminal-style text representation for each time step.\n",
       "  The text can include newlines and ANSI escape sequences (e.g. for colors).\n",
       "\n",
       "Note:\n",
       "    Rendering computations is performed internally even if you don't call render().\n",
       "    To avoid this, you can set render_mode = None and, if the environment supports it,\n",
       "    call render() specifying the argument 'mode'.\n",
       "\n",
       "Note:\n",
       "    Make sure that your class's metadata 'render_modes' key includes\n",
       "    the list of supported modes. It's recommended to call super()\n",
       "    in implementations to use the functionality of this method."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Jorgvt/matatena/blob/main/matatena/gym.py#L98){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MatatenaEnv.render\n",
       "\n",
       ">      MatatenaEnv.render ()\n",
       "\n",
       "Compute the render frames as specified by render_mode attribute during initialization of the environment.\n",
       "\n",
       "The set of supported modes varies per environment. (And some\n",
       "third-party environments may not support rendering at all.)\n",
       "By convention, if render_mode is:\n",
       "\n",
       "- None (default): no render is computed.\n",
       "- human: render return None.\n",
       "  The environment is continuously rendered in the current display or terminal. Usually for human consumption.\n",
       "- single_rgb_array: return a single frame representing the current state of the environment.\n",
       "  A frame is a numpy.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image.\n",
       "- rgb_array: return a list of frames representing the states of the environment since the last reset.\n",
       "  Each frame is a numpy.ndarray with shape (x, y, 3), as with single_rgb_array.\n",
       "- ansi: Return a list of strings (str) or StringIO.StringIO containing a\n",
       "  terminal-style text representation for each time step.\n",
       "  The text can include newlines and ANSI escape sequences (e.g. for colors).\n",
       "\n",
       "Note:\n",
       "    Rendering computations is performed internally even if you don't call render().\n",
       "    To avoid this, you can set render_mode = None and, if the environment supports it,\n",
       "    call render() specifying the argument 'mode'.\n",
       "\n",
       "Note:\n",
       "    Make sure that your class's metadata 'render_modes' key includes\n",
       "    the list of supported modes. It's recommended to call super()\n",
       "    in implementations to use the functionality of this method."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(MatatenaEnv.render)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "> Simple usage examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (0.0) | Player 2 (0.0) *\n",
      "[[0. 0. 0.]    | [[0. 0. 0.]     \n",
      " [0. 0. 0.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Rolled dice is: 6\n"
     ]
    }
   ],
   "source": [
    "env = MatatenaEnv()\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "print(f\"Rolled dice is: {obs['dice']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placing the dice in column: 2\n",
      "Player 1 (0.0) * | Player 2 (6.0)\n",
      "[[0. 0. 0.]      | [[0. 0. 6.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(f\"Placing the dice in column: {action}\")\n",
    "obs, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even perform a full game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (0.0) * | Player 2 (4.0)\n",
      "[[0. 0. 0.]      | [[0. 0. 4.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n",
      "Player 1 (2.0) | Player 2 (4.0) *\n",
      "[[2. 0. 0.]    | [[0. 0. 4.]     \n",
      " [0. 0. 0.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Player 1 (2.0) * | Player 2 (5.0)\n",
      "[[2. 0. 0.]      | [[1. 0. 4.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n",
      "Player 1 (3.0) | Player 2 (5.0) *\n",
      "[[2. 0. 1.]    | [[1. 0. 4.]     \n",
      " [0. 0. 0.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Player 1 (3.0) * | Player 2 (7.0)\n",
      "[[2. 0. 1.]      | [[1. 2. 4.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n",
      "Player 1 (5.0) | Player 2 (5.0) *\n",
      "[[2. 2. 1.]    | [[1. 0. 4.]     \n",
      " [0. 0. 0.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Player 1 (3.0) * | Player 2 (7.0)\n",
      "[[2. 0. 1.]      | [[1. 2. 4.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n",
      "Player 1 (8.0) | Player 2 (7.0) *\n",
      "[[2. 0. 1.]    | [[1. 2. 4.]     \n",
      " [0. 0. 5.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Player 1 (8.0) * | Player 2 (13.0)\n",
      "[[2. 0. 1.]      | [[1. 2. 4.]    \n",
      " [0. 0. 5.]      |  [0. 0. 6.]    \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]   \n",
      "Player 1 (10.0) | Player 2 (13.0) *\n",
      "[[2. 0. 1.]     | [[1. 2. 4.]      \n",
      " [0. 0. 5.]     |  [0. 0. 6.]      \n",
      " [0. 0. 2.]]    |  [0. 0. 0.]]     \n",
      "Player 1 (10.0) * | Player 2 (18.0)\n",
      "[[2. 0. 1.]       | [[1. 2. 4.]    \n",
      " [0. 0. 5.]       |  [5. 0. 6.]    \n",
      " [0. 0. 2.]]      |  [0. 0. 0.]]   \n",
      "Player 1 (13.0) | Player 2 (18.0) *\n",
      "[[2. 3. 1.]     | [[1. 2. 4.]      \n",
      " [0. 0. 5.]     |  [5. 0. 6.]      \n",
      " [0. 0. 2.]]    |  [0. 0. 0.]]     \n",
      "Player 1 (13.0) * | Player 2 (30.0)\n",
      "[[2. 3. 1.]       | [[1. 2. 4.]    \n",
      " [0. 0. 5.]       |  [5. 0. 6.]    \n",
      " [0. 0. 2.]]      |  [0. 0. 4.]]   \n",
      "Player 1 (22.0) | Player 2 (30.0) *\n",
      "[[2. 3. 1.]     | [[1. 2. 4.]      \n",
      " [0. 3. 5.]     |  [5. 0. 6.]      \n",
      " [0. 0. 2.]]    |  [0. 0. 4.]]     \n",
      "Player 1 (22.0) * | Player 2 (33.0)\n",
      "[[2. 3. 1.]       | [[1. 2. 4.]    \n",
      " [0. 3. 5.]       |  [5. 0. 6.]    \n",
      " [0. 0. 2.]]      |  [3. 0. 4.]]   \n",
      "Player 1 (28.0) | Player 2 (33.0) *\n",
      "[[2. 3. 1.]     | [[1. 2. 4.]      \n",
      " [0. 3. 5.]     |  [5. 0. 6.]      \n",
      " [0. 6. 2.]]    |  [3. 0. 4.]]     \n",
      "Player 1 (28.0) * | Player 2 (37.0)\n",
      "[[2. 3. 1.]       | [[1. 2. 4.]    \n",
      " [0. 3. 5.]       |  [5. 4. 6.]    \n",
      " [0. 6. 2.]]      |  [3. 0. 4.]]   \n",
      "Player 1 (29.0) | Player 2 (36.0) *\n",
      "[[2. 3. 1.]     | [[0. 2. 4.]      \n",
      " [1. 3. 5.]     |  [5. 4. 6.]      \n",
      " [0. 6. 2.]]    |  [3. 0. 4.]]     \n",
      "Player 1 (29.0) * | Player 2 (42.0)\n",
      "[[2. 3. 1.]       | [[0. 2. 4.]    \n",
      " [1. 3. 5.]       |  [5. 4. 6.]    \n",
      " [0. 6. 2.]]      |  [3. 2. 4.]]   \n",
      "Player 1 (35.0) | Player 2 (42.0) *\n",
      "[[2. 3. 1.]     | [[0. 2. 4.]      \n",
      " [1. 3. 5.]     |  [5. 4. 6.]      \n",
      " [2. 6. 2.]]    |  [3. 2. 4.]]     \n"
     ]
    }
   ],
   "source": [
    "env = MatatenaEnv()\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpu')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
