{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "from itertools import cycle\n",
    "\n",
    "import numpy as np\n",
    "from fastcore.basics import patch\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from matatena.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've been able to implement the basic functionalities of the game in Python, our next step is to implement it as a `gym.Env` so that it can be used easily to train reinforcement learning models. As a starting point, we will be following the docs: https://www.gymlibrary.dev/content/environment_creation/.\n",
    "\n",
    "They remind us to add the `metadata` attribute to specify the render-mode (`human`, `rgb_array` or `ansi`) and the framerate. Every environment should support the render-mode `None`, and you don't need to add it explicitly.\n",
    "\n",
    "As we have almost defined the environment completelly before, we don't need to add a lot of information to this class (we can inherit from the one we defined before); but we have to explicitly define the attributes `self.observation_space` and `self.action_space`.\n",
    "\n",
    "- `self.action_space`: Our agents can only choose them column in which they want to place the dice, so our action space is going to be restricted to a number between 0 and 2 (assuming the board has 3 columns, but could depend on it directly).\n",
    "\n",
    "- `self.observation_space`: What does an agent see? It makes sense to provide all the information available: Its current board, the opponent's board and the dice it has to place. We can implement this easily with a `spaces.Dict`. The different boards can be encoded as `spaces.Box` with `dtype=np.uint8` so that they are discrete environments by with an array-like shape. It should work very similarly with a `spaces.MultiDiscrete` environment for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MatatenaEnv(gym.Env, Game):\n",
    "    \"\"\"\n",
    "    `gym`-ready implementation of `Game`.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\":[\"human\"],\n",
    "                \"render_fps\":4}\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MatatenaEnv, self).__init__(**kwargs)\n",
    "        self.action_space = spaces.Discrete(self.board_size)\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(low=0,  high=6, shape=(3,3), dtype=np.uint8),\n",
    "                \"opponent\": spaces.Box(low=0,  high=6, shape=(3,3), dtype=np.uint8),\n",
    "                \"dice\": spaces.Discrete(6)\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Player 1 (0.0) | Player 2 (0.0) *\n",
       "[[0. 0. 0.]    | [[0. 0. 0.]     \n",
       " [0. 0. 0.]    |  [0. 0. 0.]     \n",
       " [0. 0. 0.]]   |  [0. 0. 0.]]    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena = MatatenaEnv()\n",
    "matatena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('agent',\n",
       "              array([[6, 1, 3],\n",
       "                     [6, 3, 6],\n",
       "                     [3, 6, 5]], dtype=uint8)),\n",
       "             ('dice', 2),\n",
       "             ('opponent',\n",
       "              array([[2, 1, 5],\n",
       "                     [1, 5, 3],\n",
       "                     [6, 2, 2]], dtype=uint8))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset\n",
    "\n",
    "The `reset` method will be called to initiate a new episode. It should be called as well when  a `done` signal is issued by the environment to reset it. It must accept a `reset` parameter. \n",
    "\n",
    "It is recommended to use the random generator included when inheriting from `gym.Env`(`self.np_random`), but we need to remember to call `super().reset(seed=seed)` to make sure that the environment is seeded correctly. \n",
    "\n",
    "Finally, it must return a tuple of the initial observation and some auxiliary information (which will be `None` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def reset(self: MatatenaEnv,\n",
    "          seed: int=None, # Seed to control the RNG.\n",
    "          options=None # Additional options.\n",
    "          ): # Initial state of the environment.\n",
    "    \"\"\"\n",
    "    Reinitializes the environment and returns the initial state.\n",
    "    \"\"\"\n",
    "    super(MatatenaEnv, self).reset(seed=seed)\n",
    "\n",
    "    self.boards = np.zeros(shape=(self.n_players, self.board_size, self.board_size))\n",
    "    self.current_player = self.choose_initial_player()\n",
    "    self._players = cycle(range(self.n_players))\n",
    "    opposite_players_mask = np.arange(self.boards.shape[0]) != self.current_player\n",
    "    self.last_dice = np.random.choice(range(1,7))\n",
    "    observation =  {\n",
    "      \"agent\": self.boards[self.current_player],\n",
    "      \"opponent\": self.boards[opposite_players_mask].squeeze(),\n",
    "      \"dice\": self.last_dice,\n",
    "    }\n",
    "    info = None\n",
    "    \n",
    "    return (observation, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Player 1 (0.0) * | Player 2 (0.0)\n",
       "[[0. 0. 0.]      | [[0. 0. 0.]   \n",
       " [0. 0. 0.]      |  [0. 0. 0.]   \n",
       " [0. 0. 0.]]     |  [0. 0. 0.]]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena = MatatenaEnv()\n",
    "matatena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': array([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       "  'opponent': array([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       "  'dice': 4},\n",
       " None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Player 1 (0.0) * | Player 2 (0.0)\n",
       "[[0. 0. 0.]      | [[0. 0. 0.]   \n",
       " [0. 0. 0.]      |  [0. 0. 0.]   \n",
       " [0. 0. 0.]]     |  [0. 0. 0.]]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matatena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step\n",
    "\n",
    "The `.step()` method contains the logic of the environment. Must accept an `action`, compute the state of the environment after applying the `action` and return a 4-tuple: `(observation, reward, done, info)`.\n",
    "\n",
    "> In our case, the `action` should be the column in which the agent wants to place the rolled dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def step(self: MatatenaEnv,\n",
    "         action, # Action to be executed on the environment. Should be the column in which the agent wants to place the dice.\n",
    "         ): # (observation, reward, done, info) tuple.\n",
    "\n",
    "    ## 1. Add the dice to the desired column\n",
    "    self.add_dice(player=self.current_player,\n",
    "                  column=action,\n",
    "                  dice=self.last_dice)\n",
    "    \n",
    "    ##Â 2. Check if the game is done\n",
    "    done = self.is_done()\n",
    "\n",
    "    ## 3. Give rewards regarding if they win or not\n",
    "    if done:\n",
    "        scores = [self.score(player) for player in range(self.n_players)]\n",
    "        reward = 1 if scores[self.current_player] == max(scores) else -1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    ## 4. Roll a new dice and change current player\n",
    "    self.last_dice = np.random.choice(range(1,7))  \n",
    "    self._change_player()\n",
    "    \n",
    "    ## 5. Build new observation\n",
    "    opposite_players_mask = np.arange(self.boards.shape[0]) != self.current_player\n",
    "    observation =  {\n",
    "      \"agent\": self.boards[self.current_player],\n",
    "      \"opponent\": self.boards[opposite_players_mask].squeeze(),\n",
    "      \"dice\": self.last_dice,\n",
    "    }\n",
    "    \n",
    "    return observation, reward, done, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Render`\n",
    "\n",
    "> Lastly, only rendering the environment is left.\n",
    "\n",
    "As we have previously built a quite decent `__repr__` method, we are going to only use that one. It would be nice to get something nicer runnig with *PyGame*, tho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def render(self: MatatenaEnv):\n",
    "    print(self.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "> Simple usage examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (0.0) | Player 2 (0.0) *\n",
      "[[0. 0. 0.]    | [[0. 0. 0.]     \n",
      " [0. 0. 0.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Rolled dice is: 6\n"
     ]
    }
   ],
   "source": [
    "env = MatatenaEnv()\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "print(f\"Rolled dice is: {obs['dice']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placing the dice in column: 2\n",
      "Player 1 (0.0) * | Player 2 (6.0)\n",
      "[[0. 0. 0.]      | [[0. 0. 6.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(f\"Placing the dice in column: {action}\")\n",
    "obs, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even perform a full game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 (0.0) * | Player 2 (4.0)\n",
      "[[0. 0. 0.]      | [[0. 0. 4.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n",
      "Player 1 (2.0) | Player 2 (4.0) *\n",
      "[[2. 0. 0.]    | [[0. 0. 4.]     \n",
      " [0. 0. 0.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Player 1 (2.0) * | Player 2 (5.0)\n",
      "[[2. 0. 0.]      | [[1. 0. 4.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n",
      "Player 1 (3.0) | Player 2 (5.0) *\n",
      "[[2. 0. 1.]    | [[1. 0. 4.]     \n",
      " [0. 0. 0.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Player 1 (3.0) * | Player 2 (7.0)\n",
      "[[2. 0. 1.]      | [[1. 2. 4.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n",
      "Player 1 (5.0) | Player 2 (5.0) *\n",
      "[[2. 2. 1.]    | [[1. 0. 4.]     \n",
      " [0. 0. 0.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Player 1 (3.0) * | Player 2 (7.0)\n",
      "[[2. 0. 1.]      | [[1. 2. 4.]   \n",
      " [0. 0. 0.]      |  [0. 0. 0.]   \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]  \n",
      "Player 1 (8.0) | Player 2 (7.0) *\n",
      "[[2. 0. 1.]    | [[1. 2. 4.]     \n",
      " [0. 0. 5.]    |  [0. 0. 0.]     \n",
      " [0. 0. 0.]]   |  [0. 0. 0.]]    \n",
      "Player 1 (8.0) * | Player 2 (13.0)\n",
      "[[2. 0. 1.]      | [[1. 2. 4.]    \n",
      " [0. 0. 5.]      |  [0. 0. 6.]    \n",
      " [0. 0. 0.]]     |  [0. 0. 0.]]   \n",
      "Player 1 (10.0) | Player 2 (13.0) *\n",
      "[[2. 0. 1.]     | [[1. 2. 4.]      \n",
      " [0. 0. 5.]     |  [0. 0. 6.]      \n",
      " [0. 0. 2.]]    |  [0. 0. 0.]]     \n",
      "Player 1 (10.0) * | Player 2 (18.0)\n",
      "[[2. 0. 1.]       | [[1. 2. 4.]    \n",
      " [0. 0. 5.]       |  [5. 0. 6.]    \n",
      " [0. 0. 2.]]      |  [0. 0. 0.]]   \n",
      "Player 1 (13.0) | Player 2 (18.0) *\n",
      "[[2. 3. 1.]     | [[1. 2. 4.]      \n",
      " [0. 0. 5.]     |  [5. 0. 6.]      \n",
      " [0. 0. 2.]]    |  [0. 0. 0.]]     \n",
      "Player 1 (13.0) * | Player 2 (30.0)\n",
      "[[2. 3. 1.]       | [[1. 2. 4.]    \n",
      " [0. 0. 5.]       |  [5. 0. 6.]    \n",
      " [0. 0. 2.]]      |  [0. 0. 4.]]   \n",
      "Player 1 (22.0) | Player 2 (30.0) *\n",
      "[[2. 3. 1.]     | [[1. 2. 4.]      \n",
      " [0. 3. 5.]     |  [5. 0. 6.]      \n",
      " [0. 0. 2.]]    |  [0. 0. 4.]]     \n",
      "Player 1 (22.0) * | Player 2 (33.0)\n",
      "[[2. 3. 1.]       | [[1. 2. 4.]    \n",
      " [0. 3. 5.]       |  [5. 0. 6.]    \n",
      " [0. 0. 2.]]      |  [3. 0. 4.]]   \n",
      "Player 1 (28.0) | Player 2 (33.0) *\n",
      "[[2. 3. 1.]     | [[1. 2. 4.]      \n",
      " [0. 3. 5.]     |  [5. 0. 6.]      \n",
      " [0. 6. 2.]]    |  [3. 0. 4.]]     \n",
      "Player 1 (28.0) * | Player 2 (37.0)\n",
      "[[2. 3. 1.]       | [[1. 2. 4.]    \n",
      " [0. 3. 5.]       |  [5. 4. 6.]    \n",
      " [0. 6. 2.]]      |  [3. 0. 4.]]   \n",
      "Player 1 (29.0) | Player 2 (36.0) *\n",
      "[[2. 3. 1.]     | [[0. 2. 4.]      \n",
      " [1. 3. 5.]     |  [5. 4. 6.]      \n",
      " [0. 6. 2.]]    |  [3. 0. 4.]]     \n",
      "Player 1 (29.0) * | Player 2 (42.0)\n",
      "[[2. 3. 1.]       | [[0. 2. 4.]    \n",
      " [1. 3. 5.]       |  [5. 4. 6.]    \n",
      " [0. 6. 2.]]      |  [3. 2. 4.]]   \n",
      "Player 1 (35.0) | Player 2 (42.0) *\n",
      "[[2. 3. 1.]     | [[0. 2. 4.]      \n",
      " [1. 3. 5.]     |  [5. 4. 6.]      \n",
      " [2. 6. 2.]]    |  [3. 2. 4.]]     \n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "env = MatatenaEnv()\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gpu')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
